{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_names = ['sentiment','id','date','query_string','user','text']\n",
    "data_path = 'D:/training.1600000.processed.noemoticon.csv'\n",
    "\n",
    "tweet_data = pd.read_csv(data_path, header=None, names=col_names, encoding=\"ISO-8859-1\").sample(frac=1) # .sample(frac=1) shuffles the data\n",
    "tweet_data = tweet_data[['sentiment', 'text']] # Disregard other columns\n",
    "print(tweet_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "allowed_chars = ' AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz0123456789~`!@#$%^&*()-=_+[]{}|;:\",./<>?'\n",
    "punct = '!?,.@#'\n",
    "maxlen = 280\n",
    "\n",
    "def preprocess(text):\n",
    "    return ''.join([' ' + char + ' ' if char in punct else char for char in [char for char in re.sub(r'http\\S+', 'http', text, flags=re.MULTILINE) if char in allowed_chars]])[:maxlen]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['text'] = tweet_data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['sentiment'] = '__label__' + tweet_data['sentiment'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directory for saving data if it does not already exist\n",
    "data_dir = './processed-data'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "# Save a percentage of the data (you could also only load a fraction of the data instead)\n",
    "amount = 0.125\n",
    "\n",
    "tweet_data.iloc[0:int(len(tweet_data)*0.8*amount)].to_csv(data_dir + '/train.csv', sep='\\t', index=False, header=False)\n",
    "tweet_data.iloc[int(len(tweet_data)*0.8*amount):int(len(tweet_data)*0.9*amount)].to_csv(data_dir + '/test.csv', sep='\\t', index=False, header=False)\n",
    "tweet_data.iloc[int(len(tweet_data)*0.9*amount):int(len(tweet_data)*1.0*amount)].to_csv(data_dir + '/dev.csv', sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from pathlib import Path\n",
    "\n",
    "corpus = NLPTaskDataFetcher.load_classification_corpus(Path(data_dir), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = corpus.make_label_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings\n",
    "\n",
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "#                    FlairEmbeddings('news-forward'),\n",
    "#                    FlairEmbeddings('news-backward')\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import DocumentRNNEmbeddings\n",
    "\n",
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer = ModelTrainer(classifier, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train('model-saves',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=8,\n",
    "              max_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "classifier = TextClassifier.load('model-saves/final-model.pt')\n",
    "\n",
    "pos_sentence = Sentence(preprocess('I love Python!'))\n",
    "neg_sentence = Sentence(preprocess('Python is the worst!'))\n",
    "\n",
    "classifier.predict(pos_sentence)\n",
    "classifier.predict(neg_sentence)\n",
    "\n",
    "print(pos_sentence.labels, neg_sentence.labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
